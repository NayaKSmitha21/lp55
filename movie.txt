import numpy as np
from keras.datasets import imdb
from keras import models
from keras import layers
from keras import optimizers
from keras import losses
from keras import metrics


import matplotlib.pyplot as plt
%matplotlib inline



# Load the data, keeping only 10,000 of the most frequently occuring words
(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = 10000)



train_data[:2]


train_labels


___________________
# Check the first label
train_labels[0]

______________________________
# Since we restricted ourselves to the top 10000 frequent words, no word index should exceed 10000
# we'll verify this below

# Here is a list of maximum indexes in every review --- we search the maximum index in this list of max indexes
print(type([max(sequence) for sequence in train_data]))

# Find the maximum of all max indexes
max([max(sequence) for sequence in train_data])
_____________________________

# Let's quickly decode a review

# step 1: load the dictionary mappings from word to integer index
word_index = imdb.get_word_index()

# step 2: reverse word index to map integer indexes to their respective words
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

# Step 3: decode the review, mapping integer indices to words
#
# indices are off by 3 because 0, 1, and 2 are reserverd indices for "padding", "Start of sequence" and "unknown"
decoded_review = ' '.join([reverse_word_index.get(i-3, '?') for i in train_data[0]])

decoded_review

_____________________________

len(reverse_word_index)

_____________________
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))    # Creates an all zero matrix of shape (len(sequences),10K)
    for i,sequence in enumerate(sequences):
        results[i,sequence] = 1                        # Sets specific indices of results[i] to 1s
    return results

# Vectorize training Data
X_train = vectorize_sequences(train_data)

# Vectorize testing Data
X_test = vectorize_sequences(test_data)
__________________________


X_train[0]

X_train.shape

_________________________
y_train = np.asarray(train_labels).astype('float32')
y_test  = np.asarray(test_labels).astype('float32')

__________________
model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))


model.compile(
    optimizer=optimizers.RMSprop(learning_rate=0.001),
    loss = losses.binary_crossentropy,
    metrics = [metrics.binary_accuracy]
)

____________________________
# Input for Validation
X_val = X_train[:10000]
partial_X_train = X_train[10000:]

# Labels for validation
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
___________________________________

history = model.fit(
    partial_X_train,
    partial_y_train,
    epochs=20,
    batch_size=512,
    validation_data=(X_val, y_val)
)

___________________

history_dict = history.history
history_dict.keys()
_______________________

# Plotting losses
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']

epochs = range(1, len(loss_values) + 1)

plt.plot(epochs, loss_values, 'g', label="Training Loss")
plt.plot(epochs, val_loss_values, 'b', label="Validation Loss")

plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss Value')
plt.legend()

plt.show()


______________________________________


# Training and Validation Accuracy

acc_values = history_dict['binary_accuracy']
val_acc_values = history_dict['val_binary_accuracy']

epochs = range(1, len(loss_values) + 1)

plt.plot(epochs, acc_values, 'g', label="Training Accuracy")
plt.plot(epochs, val_acc_values, 'b', label="Validation Accuracy")

plt.title('Training and Validation Accuraccy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

______________________
model.fit(
    partial_X_train,
    partial_y_train,
    epochs=3,
    batch_size=512,
    validation_data=(X_val, y_val)
)









# Making Predictions for testing data
np.set_printoptions(suppress=True)
result = model.predict(X_test)


result


y_pred = np.zeros(len(result))
for i, score in enumerate(result):
    y_pred[i] = np.round(score)




mae = metrics.mean_absolute_error(y_pred, y_test)
mae


# --- Predict a custom review ---
from keras.datasets import imdb
import numpy as np

# Load the word index
word_index = imdb.get_word_index()

# Encode the custom review
def encode_review(text):
    words = text.lower().split()
    encoded = [1]  # Start-of-sequence token
    for word in words:
        encoded.append(word_index.get(word, 2))  # 2 = unknown
    return encoded

# Vectorize the sequence (same as used in training)
def vectorize_sequence(sequence, dimension=10000):
    results = np.zeros(dimension)
    for index in sequence:
        if index < dimension:
            results[index] = 1.0
    return results

# Input review
review_text = "You will get A to Z all details of this scam, i may be wrong but due to this series, i don't think that i need to watch the upcoming movie big bull because 2 to 3 hours too less for explain the story. If you are at home, just watch this web series, you will not bore for even 1 second, All actors played their role excellently, i like reporter character as w"
encoded = encode_review(review_text)
vectorized = vectorize_sequence(encoded)
vectorized = np.expand_dims(vectorized, axis=0)  # batch format

# Predict sentiment
prediction = model.predict(vectorized)
print(f"Review: \"{review_text}\"")
print(f"Predicted sentiment score: {prediction[0][0]:.4f}")
print("Sentiment:", "Positive" if prediction[0][0] >= 0.5 else "Negative")
____________________________________






















